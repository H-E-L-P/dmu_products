{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the red SPIRE catalogue\n",
    "\n",
    "![HELP LOGO](https://avatars1.githubusercontent.com/u/7880370?s=100&v=4>)\n",
    "\n",
    "\n",
    "### The method we used is decsribed in Asboth et al. 2016.\n",
    "### Our method creates a 6 arcsec 500um map by using bicubic interpolation.\n",
    "### The D-map is created in the same way as Asboth et al. 2016, and we select all sources with a 3sigma detection in the D-map. We extract the flux density of the sources by using the same method as used for the standart blind cathalogue. \n",
    "\n",
    "#### The RED SPIRE sources are validated with the xxxx notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "import os\n",
    "\n",
    "import func_make_cat as fc\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.wcs import WCS\n",
    "from matplotlib.patches import Ellipse\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import units as u\n",
    "%matplotlib inline  \n",
    "from reproject import reproject_interp\n",
    "from reproject import reproject_exact\n",
    "import func_make_cat as fc\n",
    "\n",
    "\n",
    "loc = \"data_HELP_v1.0/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_names =  ['GAMA-09_SPIRE','GAMA-12_SPIRE','GAMA-15_SPIRE','HATLAS-NGP_SPIRE','HATLAS-SGP_SPIRE','SSDF_SPIRE',\\\n",
    "              'AKARI-SEP_SPIRE','Bootes_SPIRE','CDFS-SWIRE_SPIRE','COSMOS_SPIRE','EGS_SPIRE',\\\n",
    "              'ELAIS-N1_SPIRE','ELAIS-N2_SPIRE','ELAIS-S1_SPIRE','HDF-N_SPIRE','Lockman-SWIRE_SPIRE','SA13_SPIRE',\\\n",
    "              'SPIRE-NEP_SPIRE','xFLS_SPIRE','XMM-13hr_SPIRE','XMM-LSS_SPIRE','AKARI-NEP_SPIRE']\n",
    "band = ['250','350','500']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AKARI-NEP_SPIRE250_v1.0.fits\n"
     ]
    }
   ],
   "source": [
    "#reload(fc)\n",
    "for j in range(np.size(all_names)):    \n",
    "    print(all_names[j]+band[0]+'_v1.0.fits')\n",
    "    name = all_names[j]+band[0]+'_v1.0.fits'\n",
    "    hdulist250 = fits.open(loc+name)\n",
    "    \n",
    "    name = all_names[j]+band[1]+'_v1.0.fits'\n",
    "    hdulist350 = fits.open(loc+name)\n",
    "\n",
    "    name = all_names[j]+band[2]+'_v1.0.fits'\n",
    "    hdulist500 = fits.open(loc+name)\n",
    "\n",
    "    # https://reproject.readthedocs.io/en/stable/ for info about the reproject function\n",
    "    # order 1 pics the nearest pixel from the projected header\n",
    "    new_image0, footprint0 = reproject_interp(hdulist500[\"MFILT\"], hdulist250['IMAGE'].header, order = 0) \n",
    "\n",
    "    # NaN pixels in the orignal 500um \n",
    "    # These locations are needed as the Bicubic interpolation needs NaN's to be set to 0\n",
    "    bad = np.isnan(hdulist500[\"MFILT\"].data)\n",
    "    hdulist500[\"MFILT\"].data[bad] = 0 \n",
    "\n",
    "    new_image2, footprint2 = reproject_interp(hdulist500[\"MFILT\"], hdulist250['IMAGE'].header, order = 2) \n",
    "    bad = np.isnan(new_image0)\n",
    "    new_image2[bad] = np.nan\n",
    "\n",
    "    new_D = fc.do_filtering(hdulist250,hdulist500,new_image2)\n",
    "    \n",
    "    new_image2 -= np.nanmean(new_image2) # make sure the new image is mean substracted\n",
    "    Dhdu = fits.ImageHDU(header=hdulist250['IMAGE'].header,data=new_image2)\n",
    "    Dhdu.writeto('Red_map/'+all_names[j]+'_map_500_6ac.fits') \n",
    "\n",
    "    Dhdu = fits.ImageHDU(header=hdulist250['IMAGE'].header,data=new_D)\n",
    "    Dhdu.writeto('Red_map/'+all_names[j]+'_D_map.fits') \n",
    "    Dhdu = fits.open('Red_map/'+all_names[j]+'_D_map.fits')\n",
    "\n",
    "    dmin = 3*np.nanstd(Dhdu[1].data)\n",
    "    dp, rap, decp, xp, yp = fc.find_peak_red(Dhdu,dmin)\n",
    "    \n",
    "\n",
    "    \n",
    "    if all_names[j] == 'AKARI-NEP_SPIRE':\n",
    "        import pyregion\n",
    "        x_all = np.arange(0,np.shape(Dhdu[1].data)[0])\n",
    "        y_all = np.arange(0,np.shape(Dhdu[1].data)[1])\n",
    "        x_mat = np.tile(x_all,np.size(y_all))\n",
    "        y_mat = np.repeat(y_all,np.size(x_all))\n",
    "    \n",
    "        region_name = \"AKARI-NEP.reg\"\n",
    "        r = pyregion.open(region_name)\n",
    "        r250 = r.get_filter(hdulist250[1].header)\n",
    "        \n",
    "        mask = r250.inside(y_mat,x_mat)\n",
    "        Dhdu[1].data[x_mat[~mask],y_mat[~mask]] = np.nan\n",
    "        Dhdu.writeto('Red_map/'+all_names[j]+'_D_map3.fits') \n",
    "\n",
    "        dmin = 3*np.nanstd(Dhdu[1].data)\n",
    "        dp, rap, decp, xp, yp = np.array(fc.find_peak_red(Dhdu,dmin))        \n",
    "\n",
    "\n",
    "\n",
    "    w1 = WCS(hdulist250[\"NEBFILT\"].header) \n",
    "    w2 = WCS(hdulist350[\"NEBFILT\"].header)\n",
    "    w3 = WCS(hdulist500[\"MFILT\"].header)    \n",
    "    \n",
    "    x_250, y_250 = np.round(w1.wcs_world2pix(rap,  decp, 0)).astype(int)\n",
    "    x_350, y_350 = np.round(w2.wcs_world2pix(rap,  decp, 0)).astype(int)\n",
    "    x_500, y_500 = np.round(w3.wcs_world2pix(rap,  decp, 0)).astype(int)\n",
    "    \n",
    "    S250 = hdulist250[\"NEBFILT\"].data[y_250,x_250]\n",
    "    E250 = hdulist250[\"ERROR\"].data[y_250,x_250]\n",
    "    S350 = hdulist350[\"NEBFILT\"].data[y_350,x_350]\n",
    "    E350 = hdulist350[\"ERROR\"].data[y_350,x_350]\n",
    "    S500 = hdulist500[\"NEBFILT\"].data[y_500,x_500]\n",
    "    E500 = hdulist500[\"ERROR\"].data[y_500,x_500]\n",
    "\n",
    "    hdu = fits.BinTableHDU.from_columns(\\\n",
    "         [fits.Column(name='RA', array=rap, format ='F'),\n",
    "         fits.Column(name='Dec', array=decp, format='F'),\n",
    "         fits.Column(name='F_RED_pix_SPIRE_250', array=S250, format ='F'),\n",
    "         fits.Column(name='FErr_RED_pix_SPIRE_250', array=E250, format ='F'),          \n",
    "         fits.Column(name='F_RED_pix_SPIRE_350', array=S350, format ='F'),\n",
    "         fits.Column(name='FErr_RED_pix_SPIRE_350', array=E350, format ='F'),\n",
    "         fits.Column(name='F_RED_pix_SPIRE_500', array=S500, format ='F'),\n",
    "         fits.Column(name='FErr_RED_pix_SPIRE_500', array=E500, format ='F')         \n",
    "         ])\n",
    "    hdu.writeto('Red_map/'+all_names[j]+'_D_cat.fits')\n",
    "    \n",
    "    \n",
    "    hdulist250.close()\n",
    "    hdulist350.close()\n",
    "    hdulist500.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_HELP_v1.0/SPIRE-NEP_SPIRE250_v1.0.fits\n",
      "(0.0, '%')\n",
      "(4.0, '%')\n",
      "(8.0, '%')\n",
      "(12.0, '%')\n",
      "(16.0, '%')\n",
      "(20.0, '%')\n",
      "(24.0, '%')\n",
      "(28.000000000000004, '%')\n",
      "(32.0, '%')\n",
      "(36.0, '%')\n",
      "(40.0, '%')\n",
      "(44.0, '%')\n",
      "(48.0, '%')\n",
      "(52.0, '%')\n",
      "(56.00000000000001, '%')\n",
      "(60.0, '%')\n",
      "(64.0, '%')\n",
      "(68.0, '%')\n",
      "(72.0, '%')\n",
      "(76.0, '%')\n",
      "(80.0, '%')\n",
      "(84.0, '%')\n",
      "(88.0, '%')\n",
      "(92.0, '%')\n",
      "(96.0, '%')\n"
     ]
    }
   ],
   "source": [
    "for j in range(np.size(all_names)):   \n",
    "    print(loc+all_names[j]+band[0]+'_v1.0.fits')\n",
    "\n",
    "    name = all_names[j]+band[0]+'_v1.0.fits'\n",
    "    hdulist1 = fits.open(loc+name)\n",
    "    name = all_names[j]+band[1]+'_v1.0.fits'\n",
    "    hdulist2 = fits.open(loc+name)\n",
    "    name = all_names[j]+band[2]+'_v1.0.fits'\n",
    "    hdulist3 = fits.open(loc+name)\n",
    "\n",
    "    ff = fits.open('Red_map/'+all_names[j]+'_D_cat2.fits')\n",
    "    ra,dec = (ff[1].data)['RA'],(ff[1].data)['Dec']\n",
    "\n",
    "    r_1, ra_1, dec_1, S250_1, E250_1, S350_1, E350_1, S500_1, E500_1 = \\\n",
    "                fc.corr_psf_max_MF(hdulist1,hdulist2,hdulist3,ra, dec)\n",
    "    hdu = fits.BinTableHDU.from_columns(\\\n",
    "         [fits.Column(name='RA', array=ra_1, format ='F',unit='degree'),\n",
    "         fits.Column(name='Dec', array=dec_1, format='F',unit='degree'),\n",
    "         fits.Column(name='F_RED_MF_SPIRE_250', array=1000*S250_1, format ='F',unit='mJy'),\n",
    "         fits.Column(name='FErr_RED_MF_SPIRE_250', array=1000*E250_1, format ='F',unit='mJy'),\n",
    "         fits.Column(name='F_RED_MF_SPIRE_350', array=1000*S350_1, format ='F',unit='mJy'),\n",
    "         fits.Column(name='FErr_RED_MF_SPIRE_350', array=1000*E350_1, format ='F',unit='mJy'),\n",
    "         fits.Column(name='F_RED_MF_SPIRE_500', array=1000*S500_1, format ='F',unit='mJy'),\n",
    "         fits.Column(name='FErr_RED_MF_SPIRE_500', array=1000*E500_1, format ='F',unit='mJy'),\n",
    "         fits.Column(name='r', array=r_1, format ='F'),\n",
    "         fits.Column(name='RA_pix', array=(ff[1].data)['RA'], format ='F'), \n",
    "         fits.Column(name='Dec_pix', array=(ff[1].data)['Dec'], format ='F'), \n",
    "         fits.Column(name='F_RED_pix_SPIRE_250', array=1000*(ff[1].data)['F_RED_pix_SPIRE_250'], format ='F',unit='mJy'), \n",
    "         fits.Column(name='FErr_RED_pix_SPIRE_250', array=1000*(ff[1].data)['FErr_RED_pix_SPIRE_250'], format ='F',unit='mJy'), \n",
    "         fits.Column(name='F_RED_pix_SPIRE_350', array=1000*(ff[1].data)['F_RED_pix_SPIRE_350'], format ='F',unit='mJy'), \n",
    "         fits.Column(name='FErr_RED_pix_SPIRE_350', array=1000*(ff[1].data)['FErr_RED_pix_SPIRE_350'], format ='F',unit='mJy'), \n",
    "         fits.Column(name='F_RED_pix_SPIRE_500', array=1000*(ff[1].data)['F_RED_pix_SPIRE_500'], format ='F',unit='mJy'), \n",
    "         fits.Column(name='FErr_RED_pix_SPIRE_500', array=1000*(ff[1].data)['FErr_RED_pix_SPIRE_500'], format ='F',unit='mJy') \n",
    "         ])\n",
    "    hdu.writeto('Red_map/'+all_names[j]+'_D_MF_cat.fits') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HELP LOGO](https://avatars1.githubusercontent.com/u/7880370?s=75&v=4)\n",
    "\n",
    "**Authors**: Steven Duivenvoorden \n",
    "\n",
    "For a full description of the database and how it is organised in to `dmu_products` please the top level [readme](../readme.md).\n",
    "\n",
    "The Herschel Extragalactic Legacy Project, ([HELP](http://herschel.sussex.ac.uk/)), is a [European Commission Research Executive Agency](https://ec.europa.eu/info/departments/research-executive-agency_en)\n",
    "funded project under the SP1-Cooperation, Collaborative project, Small or medium-scale focused research project, FP7-SPACE-2013-1 scheme, Grant Agreement Number 607254.\n",
    "\n",
    "[Acknowledgements](http://herschel.sussex.ac.uk/acknowledgements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
